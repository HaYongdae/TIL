** 예시 코드 - scala 기반으로 작성됨



# SPARK ML (machine learning)

> spark는 파이썬, 자바, 스칼라를 이용한 ML_LIB을 지원한다. (데이터는 double 타입만 사용한다)
>
> **spark.mllib (스파크 RDD 기반 API) / spark.ml (스파크 DF 기반 API)**



## 1. Data Type



- Spark 고유 data Type은 다음의 [링크]([https://github.com/HaYongdae/TIL/blob/master/SPARK/%EC%8A%A4%ED%8C%8C%ED%81%AC%20SQL.md](https://github.com/HaYongdae/TIL/blob/master/SPARK/스파크 SQL.md)) 참조

- **벡터(Vector)** _ *Feature 변수*

  - Vector는 주로 데이터 표현의 방법이다.

  - 서드파티 (스칼라 - 브리즈, 자바 - MTJ, 파이썬 - NUMPY) 를 활용하여 수학적 처리도 가능하다.  이는 MLlib의 복잡성을 낮추려는 의도이다.

    

  - **고밀도 벡터(Dense vector)** 

     부동 소수의 배열에 모든 값이 저장되는 벡터

    > EX_ 사이즈 100의 벡터는 100개의 double 값을 저장

  - **저밀도 벡터(Sparse vector)**  

     공간을 절약하기 위해 0이 아닌 값만 저장하는 벡터

    > EX_ 오직 0이 아닌 값과 그것들의 인덱스를 저장

    

  - 0이 아닌 값이 10퍼센트 이하면 저밀도 벡터가 더 선호되는 편이다 

    (메모리 사용량이나 속도 측면에서).

    

- **Label** _ *Target*

  - 지도 학습에서 입력에 대한 올바른 출력 값

  > EX _ result : 1(남자), 0(여자) 일 때 도출 된 1 또는 0값

  

- **LabeledPoint** 
  - 입력값(feature)과 그에 따른 올바른 출력값(label)의 쌍 
  - (스파크 고유 타입) , 형태 => (feature, label)

---



## 2. 파이프 라인 ( pipe_line)

- 여러 알고리즘을 순차적으로 실행할 수 있게 지원하는 고차원 API - 워크 플로우를 생성

- 데이터 프레임을 사용

- Transformer – org.apache.spark.ml의 추상 클래스. 변형된 새로운 데이터프레임 생성 용도

- Estimator - org.apache.spark.ml의 추상 클래스. 데이터프레임에 적용하여 트랜스포머 생성

- Pipeline - org.apache.spark.ml의 클래스. 여러 개의 파이프라인 스테이지(PipelineStage)로 구성 
- ParamMap - 평가자나 트랜스포머에 파라미터를 전달하기 위한 목적으로 사용되는 클래스



<u>**생성 예시 코드**</u>

```scala
// 파이프라인    
val pipeline = new Pipeline().setStages(Array(assembler, lr))    
// 파이프라인 모델 생성    
val pipelineModel = pipeline.fit(training)    
// 파이프라인 모델을 이용한 예측값 생성    
pipelineModel.transform(training).show()    
```



## 3. 실습 예제



### (1) svm + pipeline 간단 예제

```scala
=========LabeledPoint 타입 생성 실습 ================
import org.apache.spark.ml.feature.LabeledPoint
import org.apache.spark.ml.linalg.Vectors
import org.apache.spark.mllib.util.MLUtils

//파일을 이용한 SparseVector  생성
///usr/local/spark/data/mllib/sample_libsvm_data.txt파일의 데이터 저장 형식
//label index1:value1 index2:value2...

val path = "file:///usr/local/spark/data/mllib/sample_libsvm_data.txt"
val v6 = MLUtils.loadLibSVMFile(sc, path)
val lp1 = v6.first

println(s"label:${lp1.label}, features:${lp1.features}")


##########Pipeline실습#######################
import org.apache.spark.ml.{Pipeline, PipelineModel}
import org.apache.spark.ml.classification.{LogisticRegression, LogisticRegressionModel}
import org.apache.spark.ml.feature.VectorAssembler
//import org.apache.spark.sql.SparkSession
//object PipelineSample {  
//   def main(args: Array[String]) {    
//val spark = SparkSession.builder().appName("PipelineSample") .master("local[*]").getOrCreate()

// 훈련용 데이터 (키, 몸무게, 나이, 성별)
val training = spark.createDataFrame(Seq((161.0, 69.87, 29, 1.0),(176.78, 74.35, 34, 1.0),(159.23, 58.32, 29, 0.0))).toDF("height", "weight", "age", "gender") 
training.cache() 
 // 테스트용 데이터    
val test = spark.createDataFrame(Seq((169.4, 75.3, 42),(185.1, 85.0, 37),(161.6, 61.2, 28))).toDF("height", "weight", "age")    
training.show(false) 
   
//height, weight, age컬럼은 예측을 위한 특성으로 사용하고, gender는 label로 사용합니다.
//height, weight, age 특성만으로 구성된 벡터 생성
//VectorAssembler는 특성변환 알고리즘 클래스
val assembler = new VectorAssembler().setInputCols(Array("height", "weight", "age")).setOutputCol("features")    

// training 데이터에 features 컬럼 추가    
val assembled_training = assembler.transform(training)  
assembled_training.show(false)   
 
// 모델 생성 알고리즘 (로지스틱 회귀 평가자 - 이진분류 알고리즘)    
val lr = new LogisticRegression().setMaxIter(10).setRegParam(0.01).setLabelCol("gender") 

// 모델 생성    
val model = lr.fit(assembled_training)    
// 예측값 생성    
model.transform(assembled_training).show()    
// 파이프라인    
val pipeline = new Pipeline().setStages(Array(assembler, lr))    
// 파이프라인 모델 생성    
val pipelineModel = pipeline.fit(training)    
// 파이프라인 모델을 이용한 예측값 생성    
pipelineModel.transform(training).show()    
val path1 = "/Users/beginspark/Temp/regression-model"    
val path2 = "/Users/beginspark/Temp/pipelinemodel"    
// 모델 저장    
model.write.overwrite().save(path1)    
pipelineModel.write.overwrite().save(path2)    
// 저장된 모델 불러오기   
val loadedModel = LogisticRegressionModel.load(path1)    
val loadedPipelineModel = PipelineModel.load(path2)    
spark.stop
```



### (2) linear regression - (날씨 - 매출 모델)

- [평일과 주말]로 데이터를 분리, 매출 예측



 사전 준비 - weather.csv, sales.csv hdfs에  save  (csv 파일은 동위 디렉토리의 ./data 참조)

```bash
hadoop fs -mkdir /data/sales
hadoop fs -put weather.csv  /data/sales/
hadoop fs -put sales.csv  /data/sales/
```



1. (weather.csv, sales.csv) 활용을 위한 case 정의

```scala
case class Weather( date: String,
                    day_of_week: String,
                    avg_temp: Double,
                    max_temp: Double,
                    min_temp: Double,
                    rainfall: Double,
                    daylight_hours: Double,
                    max_depth_snowfall: Double,
                    total_snowfall: Double,
                    solar_radiation: Double,
                    mean_wind_speed: Double,
                    max_wind_speed: Double,
                    max_instantaneous_wind_speed: Double,
                    avg_humidity: Double,
                    avg_cloud_cover: Double)
case class Sales(date: String, sales: Double)
```



2. csv load - RDD 생성

```scala

import spark.implicits._
import org.apache.spark.mllib.regression.{LabeledPoint,LinearRegressionWithSGD}
import org.apache.spark.mllib.linalg.Vectors
import org.apache.spark.mllib.feature.StandardScaler
import org.apache.spark.mllib.evaluation.RegressionMetrics
import org.apache.spark.sql.functions.udf



// 기상 데이터를 읽어 DataFrame으로 변환한다
val weatherCSVRDD = sc.textFile("/data/sales/weather.csv")
val headerOfWeatherCSVRDD = sc.parallelize(Array(weatherCSVRDD.first))
val weatherCSVwithoutHeaderRDD = weatherCSVRDD.subtract(headerOfWeatherCSVRDD)


// 매출 데이터를 읽어 RDD 생성
val salesCSVRDD = sc.textFile("/data/sales/sales.csv")
val headerOfSalesCSVRDD = sc.parallelize(Array(salesCSVRDD.first))
val salesCSVwithoutHeaderRDD = salesCSVRDD.subtract(headerOfSalesCSVRDD)
```



3.  RDD => DF   ( 생성 성공 여부는 **DF객체.printSchema** 혹은 **.show()** 활용)

```scala
// weatherDF
val weatherDF = weatherCSVwithoutHeaderRDD.map(_.split(",")).
      map(p => Weather(p(0),
      p(1),
      p(2).trim.toDouble,
      p(3).trim.toDouble,
      p(4).trim.toDouble,
      p(5).trim.toDouble,
      p(6).trim.toDouble,
      p(7).trim.toDouble,
      p(8).trim.toDouble,
      p(9).trim.toDouble,
      p(10).trim.toDouble,
      p(11).trim.toDouble,
      p(12).trim.toDouble,
      p(13).trim.toDouble,
      p(14).trim.toDouble
    )).toDF()


// salesDF
val salesDF = salesCSVwithoutHeaderRDD.map(_.split(",")).map(p => Sales(p(0), p(1).trim.toDouble)).toDF()
```



4. salesDF 와 weatherDF JOIN

```scala
//  'DF.join(DF, "기준 컬럼")'
val salesAndWeatherDF = salesDF.join(weatherDF, "date")
```



5. 평일과 주말을 나누는 UDF(사용자 정의 함수)생성 => DF에 적용

```scala
val isWeekend = udf((t: String) => if(t.contains("일") || t.contains("토")) 1d else 0d)
val replacedSalesAndWeatherDF = salesAndWeatherDF.withColumn("weekend", isWeekend(salesAndWeatherDF("day_of_week"))).drop("day_of_week")
```



6. 필요한 컬럼만 분리

```scala
val selectedDataDF = replacedSalesAndWeatherDF.select("sales", "avg_temp", "rainfall", "weekend")
```



7.  ML 데이터 타입 import

```scala
import org.apache.spark.ml.feature.LabeledPoint
```



8. DF => LabeledPoint

```scala
val labeledPointsRDD = selectedDataDF.rdd.map(row => LabeledPoint(row.getDouble(0),
 Vectors.dense(row.getDouble(1),row.getDouble(2),row.getDouble(3))))

```



9. 데이터 정규화(정규분포화) 메소드 생성 => LabeledPoint 정규화

```scala
val scaler = new StandardScaler().fit(labeledPointsRDD.map(x =>x.features))
val scaledLabledPointsRDD = labeledPointsRDD.map(x => LabeledPoint(x.label, scaler.transform(x.features)))

```



10. 선형회귀 모델을 작성한다.

```scala
val numIterations = 20
scaledLabledPointsRDD.cache
val linearRegressionModel = LinearRegressionWithSGD.train(scaledLabledPointsRDD, numIterations)

println("weights :" + linearRegressionModel.weights)
```



11. test 데이터 확인

```scala
// 데이터 생성
val targetDataVector1 = Vectors.dense(15.0,15.4,1)
val targetDataVector2 = Vectors.dense(20.0,0,0)
// 정규화
val targetScaledDataVector1 = scaler.transform(targetDataVector1)
val targetScaledDataVector2 = scaler.transform(targetDataVector2)

// 예측결과 출력
val result1 = linearRegressionModel.predict(targetScaledDataVector1)
println("avg_tmp=15.0,rainfall=15.4,weekend=true : sales = " + result1)

val result2 = linearRegressionModel.predict(targetScaledDataVector2)
println("avg_tmp=20.0,rainfall=0,weekend=false : sales = " + result2)
```



12. 데이터 분할, 평가 및 모델 저장

```scala
 val splitScaledLabeledPointsRDD = scaledLabledPointsRDD.randomSplit(Array(0.6, 0.4), seed = 11L)
val trainingScaledLabeledPointsRDD = splitScaledLabeledPointsRDD(0).cache()
val testScaledLabeledPointsRDD = splitScaledLabeledPointsRDD(1)
val linearRegressionModel2 = LinearRegressionWithSGD.train(trainingScaledLabeledPointsRDD, numIterations)
val scoreAndLabels = testScaledLabeledPointsRDD.map { point =>
    val score = linearRegressionModel2.predict(point.features)
      (score, point.label)
    }

val metrics = new RegressionMetrics(scoreAndLabels)
println("RMSE = "+ metrics.rootMeanSquaredError)
// 작성한 모델을 보존한다
linearRegressionModel.save(sc, "/output/mllib/model/")
 
import org.apache.spark.mllib.regression.LinearRegreesionModel
val model2 = LinearRegressionModel.load(sc, "/output/mllib/model/")

linearRegressionModel.toPMML("model.pmml")
```





코드 통합본

```scala
case class Weather( date: String,
                    day_of_week: String,
                    avg_temp: Double,
                    max_temp: Double,
                    min_temp: Double,
                    rainfall: Double,
                    daylight_hours: Double,
                    max_depth_snowfall: Double,
                    total_snowfall: Double,
                    solar_radiation: Double,
                    mean_wind_speed: Double,
                    max_wind_speed: Double,
                    max_instantaneous_wind_speed: Double,
                    avg_humidity: Double,
                    avg_cloud_cover: Double)
case class Sales(date: String, sales: Double)


import spark.implicits._
import org.apache.spark.mllib.regression.{LabeledPoint,LinearRegressionWithSGD}
import org.apache.spark.mllib.linalg.Vectors
import org.apache.spark.mllib.feature.StandardScaler
import org.apache.spark.mllib.evaluation.RegressionMetrics
import org.apache.spark.sql.functions.udf



// 기상 데이터를 읽어 DataFrame으로 변환한다
val weatherCSVRDD = sc.textFile("/data/sales/weather.csv")
val headerOfWeatherCSVRDD = sc.parallelize(Array(weatherCSVRDD.first))
val weatherCSVwithoutHeaderRDD = weatherCSVRDD.subtract(headerOfWeatherCSVRDD)
val weatherDF = weatherCSVwithoutHeaderRDD.map(_.split(",")).
      map(p => Weather(p(0),
      p(1),
      p(2).trim.toDouble,
      p(3).trim.toDouble,
      p(4).trim.toDouble,
      p(5).trim.toDouble,
      p(6).trim.toDouble,
      p(7).trim.toDouble,
      p(8).trim.toDouble,
      p(9).trim.toDouble,
      p(10).trim.toDouble,
      p(11).trim.toDouble,
      p(12).trim.toDouble,
      p(13).trim.toDouble,
      p(14).trim.toDouble
    )).toDF()

// 매출 데이터를 읽어 DataFrame으로 변환한다
val salesCSVRDD = sc.textFile("/data/sales/sales.csv")
val headerOfSalesCSVRDD = sc.parallelize(Array(salesCSVRDD.first))
val salesCSVwithoutHeaderRDD = salesCSVRDD.subtract(headerOfSalesCSVRDD)
val salesDF = salesCSVwithoutHeaderRDD.map(_.split(",")).map(p => Sales(p(0), p(1).trim.toDouble)).toDF()

//정의된 스키마 확인
println(weatherDF.printSchema)  
println(salesDF.printSchema)   


// 데이터의 전처리(날짜 기준으로 조인 후, 요일 컬럼값을 수치화하고, 요일컬럼제거후 , 수치화된 주말컬럼 추가)
val salesAndWeatherDF = salesDF.join(weatherDF, "date")
val isWeekend = udf((t: String) => if(t.contains("일") || t.contains("토")) 1d 
                                       else 0d)
val replacedSalesAndWeatherDF = salesAndWeatherDF.withColumn("weekend", isWeekend(salesAndWeatherDF("day_of_week"))).drop("day_of_week")

//매출에 영향을 주는 독립변수만 추출하여 새로운 데이터 프레임 생성
//매출에 영향을 주는 독립변수 평균기온, 일강수량, 휴일을 선택
val selectedDataDF = replacedSalesAndWeatherDF.select("sales", "avg_temp", "rainfall", "weekend")

//데이터프레임을 회귀분석을 위한 Vector, LabeledPoint로 생성
 val labeledPointsRDD = selectedDataDF.rdd.map(row => LabeledPoint(row.getDouble(0),
 Vectors.dense(row.getDouble(1),row.getDouble(2),row.getDouble(3))))

//데이터 특성을 표준화(평균 0, 분산1인 스케일러 사용)
// 데이터의 표준화 (평균값을 조정하고 스케이링을 개별적으로 유효화 또는 무효화를 할 수 있다
//val scaler = new StandardScaler(withMean = true, withStd = true).fit(labeledPointsRDD.map(x => x.features))

val scaler = new StandardScaler().fit(labeledPointsRDD.map(x =>x.features))
val scaledLabledPointsRDD = labeledPointsRDD.map(x => LabeledPoint(x.label, scaler.transform(x.features)))


// 선형회귀 모델을 작성한다
    val numIterations = 20
    scaledLabledPointsRDD.cache
    val linearRegressionModel = LinearRegressionWithSGD.train(scaledLabledPointsRDD, numIterations)
    println("weights :" + linearRegressionModel.weights)

// 알고리즘에 미지의 데이터를 적용해 예측한다
    val targetDataVector1 = Vectors.dense(15.0,15.4,1)
    val targetDataVector2 = Vectors.dense(20.0,0,0)
    val targetScaledDataVector1 = scaler.transform(targetDataVector1)
    val targetScaledDataVector2 = scaler.transform(targetDataVector2)
    val result1 = linearRegressionModel.predict(targetScaledDataVector1)
    val result2 = linearRegressionModel.predict(targetScaledDataVector2)
		// 
    println("avg_tmp=15.0,rainfall=15.4,weekend=true : sales = " + result1)
		// avg_tmp=15.0,rainfall=15.4,weekend=true : sales = 724998.3949513528
	println("avg_tmp=20.0,rainfall=0,weekend=false : sales = " + result2)
		// avg_tmp=20.0,rainfall=0,weekend=false : sales = 444132.5080443638

 val splitScaledLabeledPointsRDD = scaledLabledPointsRDD.randomSplit(Array(0.6, 0.4), seed = 11L)
    val trainingScaledLabeledPointsRDD = splitScaledLabeledPointsRDD(0).cache()
    val testScaledLabeledPointsRDD = splitScaledLabeledPointsRDD(1)
    val linearRegressionModel2 = LinearRegressionWithSGD.train(trainingScaledLabeledPointsRDD, numIterations)
    val scoreAndLabels = testScaledLabeledPointsRDD.map { point =>
     val score = linearRegressionModel2.predict(point.features)
      (score, point.label)
    }

val metrics = new RegressionMetrics(scoreAndLabels)
    println("RMSE = "+ metrics.rootMeanSquaredError)
    // 작성한 모델을 보존한다
linearRegressionModel.save(sc, "/output/mllib/model/")
 
import org.apache.spark.mllib.regression.LinearRegressionModel
val model2 = LinearRegressionModel.load(sc, "/output/mllib/model/")

linearRegressionModel.toPMML("model.pmml")
```

