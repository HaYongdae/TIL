# ADP Study



## 1단원 - 데이터의 이해

### 1-1장 - 데이터의 이해

#### 1-1-1절 데이터와 정보 (DIKW 정의)

**1. Data**

> 1646년 영국 문헌 첫 등장, 추론과 추정의 근거를 이루는 사실(옥스포드 대사전) 
>
> 데이터는 지식경영의 핵심인 암묵지의 형식지화에 역할을 담당한다 (Polany, 1966)

- 특성 - 존재적 (객관적 사실) / 당위적 (추론, 예측, 전망, 추정을 위한 근거)

- 유형 - 정성적 데이터 (언어, 문자) / 정량적 (수치, 도형, 기호 등)

**2. 정보**

> 데이터의 가공, 처리와 데이터간 연관관계 속에서 의미가 **도출된 것**

**3.지식**

> 도출된 정보를 구조화하여 유의미한 정보를 분류하고 경험을 결합시켜 **내재화한 것**

**4. 지혜**

> 지식의 축적과 아이디어가 결합된 창의적인 산물



#### 1-1-2절 DB의 정의 및 특징



**1. DataBase**

> 50년대 : 미군의 관리를 위한 컴퓨터 도서관을 설립 - 데이터의 기지라는 뜻의 DataBase 탄생
>
> 63년 : 미국 SDC가 개회한 심포지엄에서 공식 사용 '대량의 데이터를 축적하는 기지'의 의미
>
> 65년 : 2차 심포지엄에서 '시스템을 통한 체계적 관리와 저장 의미' 포함한 DB System 표현 등장
>
> 75년 : CAC가 KORSTIC을 통해 서비스됨. 우리나라에 DB  활용 시작 / TECHNOLINE 정보검색 서비스를 개시

- 정의 

  - 1차 개념 : 정형데이터의 관리
    - EU : 체계적, 조직적으로 정리되고 전자식 또는 기타 수단으로 개별적으로 접근할 수 있는 독립된 저작물, 데이터 또는 기타 소재의 수집물
    - 국내 : 소재를 체계적으로 배열 구성한 편집물. 개별적으로 접근하거나 검색할 수 있도록 한 것.
  - 2차 개념 : 빅데이터의 등장 (비정형 포함)
    - 국내 - 컴퓨터 용어사전 : 동시에 복수 업무 지원. 복수 이용자에 대응해 데이터를 받아들이고, 저장, 공급하기 위한 일정한 구조로 편성된 데이터의 집합
    - 국내 - 데이터분석 전문가 가이드 : 문자, 기호, 음성, 화상, 영상 등 상호 관련된 다수의 콘텐츠를 정보 처리 및 정보통신 기기에 의하여 체계적으로 수집, 축적하여 다양한 용도와 방법으로 이용할 수 있도록 정리한 정보의 집합체

- 특징

  - 통합된 (Integrated) - 중복 x
  - 저장된 (stored) - 저장 매채에 저장
  - 공용의 (shared) - 복수의 사용자가 복수의 목적으로 사용
  - 변화가능한 (changable) 

  - 정보 축적 및 전달 측면 : 기계가독성, 검색가독성, 원격조작성
  - 관리 및 이용 측면 : 일정한 질서와 구조에 따라 관리, 경제적
  - 정보기술 발전 측면 : 네트워크 기술의 발전에 이바지

#### 1-1-3절 DB 활용

- OLTP(On-line Transaction Processing) - 다양한 실시간 트랜젝션 데이터의 처리 프로세스
- OLAP(On-line Analytical Processing) - 요약된 데이터를 주제 중심적으로 활용하여 인사이트를 얻는 과정



### 1-2장 - 데이터의 가치와 미래

#### 1-2-1절 빅데이터의 이해

> 좁게는 3V의 특성을 가진 데이터를 의미, 기술과 분석까지 포함하거나 인재, 조직까지도 포함하는 단어 
>
> 4V - Volume(량), Variaty(다양성), Velocity(속도) + Value(가치), Visualization(시각화), Veracity(정확성)

#### 1-2-2절 빅데이터의 가치와 영향

> 맥킨지 曰 : 1. 투명성 제고(관리 효율 증대), 2. 시뮬레이션을 통한 예측(경쟁력 강화), 3. 세분화(맞춤 서비스), 					4. 알고리즘 활용 의사결정(경제성 증대), 5. BM과 서비스의 혁신

#### 1-2-3절 Business Model

> 연관규칙학습(상관관계), 유형분석(분류), 기계학습, 회귀분석(관계의 정량화), 감정분석, SNS분석 등

#### 1-2-4절 위기 요인과 통제 방안

> 위기 요인 : 사생활 침해, 책임원칙훼손(발생하지 않은 현상에 대한 확신), 데이터 오용
>
> 통제 방안 : 동의에서 책임으로, 결과 기반 책임 원칙, 알고리즘 접근 허용 



### 1-3장 - 데이터 사이언스와 전략 인사이트

#### 1-3-1절 분석과 전략 인사이트

> 과거의 대규모 투자에서 실패한 경험들이 아직 고착화되어 있음.
>
> 일차적인 분석보다는 전략적, 점증적 분석 전략을 활용해야함. 

#### 1-3-2절 인사이트 도출 필요 역량

> 데이터, 수학, 통계학, 컴퓨터공학, 시각화, 도메인 지식 등이 필요
>
> Analytics(분석), 컨설팅(스토리텔링, 시각화), IT 의 복합적 역량 필요 (Hard and Soft)

#### 1-3-3절 미래



## 2장 - 데이터 처리 기술의 이해

### 2-1장 - 데이터 처리 프로세스

#### 2-1-1절 ETL

> 데이터 이동 및 변환 (Extraction, Transformation, Load)
>
> ODS(operational Data Source)와 DW, DM을 만드는 과정을 의미
>
> Integration, Migration, Master Data Management에 활용되며 주기적 재사용이 가능한 컴포넌트들로 구성
>
> MPP(Masive Parallel Processing)를 활용 - 프로그램을 파트별로 나누어 병렬 처리하는 형식
>
> Batch ETL(일괄), Real Time ETL(실시간)로 구분

- 과정 : 스테이징 - 프로파일링(품질 측정, 특성 파악) - 클렌징 - integration - DeNormalizing - report table

![Process](.\img\Process.jpg)



** ODS란 ? ETL 과정을 위해 추출, 통합한 DB / DW로 이관됨 / 클렌징, 중복제거, 무결성 검증 등 수행 

​					/ Real Time 데이터를 관리할 목적으로 설계 

- ODS 생성과정
  - 인터페이스 단계
    - 원천으로 부터 데이터를 획득하는 과정
    - OLEDB(Object Linking and Embedding DataBase), ODBC(Object DataBase Connectivity), FTP(File Transfer Protocol) 등의 프로토콜 활용
  - 스테이징 단계
    - 스테이징 테이블에 저장되는 단계
    - 원천의 스키마에 의존하여 원본과의 일대일 혹은 일대다 형식의 테이블을 로드
    - RDB, 스프레드시트, Web, XML, 트랜젝션 데이터 등의 형태를 지님
  - 프로파일링 단계
    - 범위, 도메인, 유일성 확보 등의 규칙을 기준으로 데이터의 품질을 점검
    - 데이터 품질 보고서를 작성하는데 사용
  - 클렌징 단계
    - 오류 데이터들을 수정하는 단계
  - 인티그레이션 단계
    - 수정 완료 데이터를 ODS에 적재하는 단계
  - 익스포트
    - 보안 및 익스포트 규칙을 반영하여 DW, DM 등을 생성

** DW 란? ODS를 통해 정제 및 통합된 데이터가 분석 보고서 생성을 위해 적재되는 DB

​					주제중심적(최종사용자도 이해할 수 있는 형태를 지님), 영속성/비휘발성, 통합성, 시계열성

- DW의 형태 : 스타 스키마 (Fact - 3차원 정규화 , Dim - 2차원 정규화) - 중복 多, 조인 少, 구조 이해 쉬움

  ​					눈송이 스키마 (Fact, Dim 모두 3차원 정규화) - 중복 少, 조인 多, 구조 복잡



#### 2-1-2절 CDC (change Data Capture)

> 데이터 변경을 식별하고 후속처리를 자동화하는 기술 / 설계 기법
>
> 실시간 / 근접 실시간 서비스를 위해 구축 (Storage부터 App까지 다양한 계층에서 구축 가능)



- CDC 구현 기법 (푸쉬 방식과 풀 방식 중 선택)

  - Time Stamp on Row - Updata Time을 기록하고 체크

  - Version Number on Row - 버전을 기록하고 체크

  - Status on Row - 데이터 변경을 Boolean 값으로 기록하고 체크

  - Triggers on Table - 사전에 설정된 Trigger를 활용 (복잡도 증가, 변경 어려움, 확장성 감소로 주의 필요)

  - Event Programming - 변경 식별 기능을 App으로 구현하여 체크 - 복잡한만큼 복잡한 체크 가능

  - Log Scanner on DataBase - DB 트랜젝션 로그에 스캐닝을 구현 

    (스키마 변경 불필요, But DB의 종류가 다양할 경우 업무 복잡도가 높아질 수 있음)



#### 2-1-3절 EAI (Enterprise Application Integration)

> 비즈니스 프로세스 기반의 각종 Application을 상호연동을 하는 통합솔루션
>
> 데이터를 연계함으로서 동기화되도록 함
>
> Front_Office Sys, 레거시 Sys, 패키지 App 등의 형태로 산재된 App 프로세스 및 메시지를 통합 및 관리
>
> ETL이 배치 프로세스 중심이라면, EAI는 실시간 or 근접 실시간 위주
>
> Point To Point(복잡도 : N(N-1)/2개)를 Hub And Spoke(복잡도 : n) 방식으로 연계

- 구성
  - Adapter : 각 정보 시스템과 EAI 허브의 연결성을 확보
  - Bus : 연동 경로
  - Broker : 데이터 연동 규칙 통제
  - Transformer : 데이터 형식 변환 담당
- 구현 유형
  - Mediation(intra-communication)
    - EAI 엔진이 중개자로 동작하며, 데이터 갱신 및 생성 / 트랜젝션 완료 이벤트를 식별하여 동작
    - Publish / subscribe Model
  - Federation(inter-communication)
    - EAI 엔진이 외부 시스템으로부터 데이터 요청들을 일괄적으로 수령하여 필요한 데이터 전달
    - Repuest / Reply Model

** ESB(Enterprise Service Bus)와의 차이점

- Application 위주와 Process 위주의 차이 , 허브시스템 중앙집중형과 버스 형태의 느슨한 연결구조 차이



#### 2-1-4절 데이터 통합 및 연계 기법

> 모니터링이 필요한 센서 데이터 - Real Time, 대용량 전처리용 - Batch



#### 2-1-5절 대용량 비정형 데이터 처리

- Log 
  - 양이 방대, 오류 분석부터 마케팅까지 활용
  - 아파치 Flume NG, Facebook의 Scribe, 아파치 Chukwa 등 활용)
    - 실시간, 대상 서버 수만큼의 확장성 등 유연하고 고성능의 리소스 필요
    - 데이터 전송 시 보장이 무엇보다 중요. 성능과 안정성의 Trade-Off를 통해 알맞는 방식 설계 필요
    - 수집과 저장의 내장 플러그인을 제공 (하둡, NoSQL 저장 기능 포함)
    - 제공 기능의 일부 수정 용이

- 대규모 분산 병렬 처리
  - 하둡

### 2-2장 - 데이터 처리 기술

#### 2-2-1절 분산 데이터 저장 기술

> 분산 파일 시스템, 클러스터, DB, NoSQL

##### 1. 분산 파일 시스템

> 분산된 서버들로 구성, 대규모 클러스터 시스템 플랫폼 : 저장 공간, 처리 성능, 확장성, 신뢰성, 가용성 확보
>
> 파일의 메타데이터를 관리하는 전용 서버 + 데이터 저장 처리 서버 : 비대칭형 클러스터 파일 시스템

- GFS (Google File System) - 64MB 청크로 분산, 헤시 테이블 구조 활용, 마스터에 의해 생성/삭제 구별
  - 가정 : 고장이 빈번한 저가형 서버 다수 운용, 쓰기가 순차적이며 갱신은 드뭄. 실시간보다 높은 처리율
  - 클라이언트(POSIX 지원 X) - 마스터(메타데이터를 메모리 상에서 처리) - 청크 서버(하트비트 송출) 
  - 클라이언트는 마스터에게 핸들을 받고 청크 서버에 직접 요청
- HDFS (Hadoop File System) - 아파치 너치 프로젝트를 클로닝, 128MB 블록 구성
  - 네임노드 - 데이터노드 - 보조네임노드
  - 순차적 스트리밍 방식으로 저장 조회(노드 1 완료 후 2실행, 3실행 순), 배치 작업에 적합 
  - 클라이언트가 마스터에게 정보를 제공받아 직접 데이터를 읽어 들임.
- 러스터 - 객체 기반 클러스터 파일 시스템
  - 고속 네트워크로 연결된 클라이언트 서버(리눅스 VFS) - 메타 서버 - 객체 저장 서버(스트라이핑 기법 사용)
  - 계층화된 모듈 구조로 TCP/IP, 인피니밴드, 미리넷을 지원
  - 유닉스 시멘틱 제공, 라이트백 캐시 지원, 클라이언트에서 갱신 레코드를 생성, 무결성을 위해 잠금 활용
  - 인텐트 기반 잠금 프로토콜 활용

##### 2. DB 클러스터

> 클러스터링 - 하나의 DB를 여러 서버에 분산 배치, 파티셔닝 - 하나의 DB를 분할하여 처리하는 방식
>
> 병렬처리, 고가용성을 위해 활용, 무공유/공유 디스크로 구분

- 무공유 디스크 - 대부분의 클러스터가 채택, 서로 완전히 분리되어 서로의 데이터에 소유권 X
  - 노드확장에 제한이 없으나 폴트톨로런스를 구성해야함
- 공유 디스크 - 각 인스턴스가 모든 데이터에 접근 가능, SAN(Storage Area Network)등의 활용
  - 폴트톨로런스를 제공(하나의 서버만 살아도 동작), 확장이 제한적이고 병목이 발생, 동기화 방식 고민 필요



----

----

띄어 넘음

---



## 3단원 - 데이터 분석 기획

### 3-1장 - 데이터 분석 기획의 이해

#### 3-1-1절 분석기획 방향성 도출

> IT, 수리통계, 도메인 지식을 기반으로 What, Why, How를 구상.
>
> 대상O 분석O - Optimization, 대상X, 분석O - Insight, 대상O, 분석X - Solution, 대상X, 분석X - Discovery
>
> 가용 데이터, 목적, 한계 등을 최대한 고려하여 작성

#### 3-1-2절 분석 방법론

> 방법론 - 절차, 방법, 도구와 기법, 템플릿과 산출물로 구성
>
> 암묵지 (형식화) 형식지 (체계화) 방법론 (내재화) 암묵지 순환 [공통화, 내면화] [표출화, 연결화]



##### 1. 방법론의 적용 모델

- 폭포수 모델 - 단계를 순차적으로 수행하는 모델 

- 프로토타입 모델 - 고객 요구가 뚜렷하지 않을 때 활용. 프로토타입을 재공하고 개선해나가는 방식

- 나선형 모델 - 점증적 개발. 바탕이 없는 상황에서 활용가능하지만 체계가 잡히지 않으면 어려움을 동반



##### 2. KDD 분석

- 1996년 Fayyad가 프로파일링 기술을 기반으로 정리 (Knowledge Discovery in DataBases)
  - Selection - 도메인 및 프로젝트 목표 이해, 목표 데이터의 선정
  - PreProcessing - Noise, Outlier, Missing Value 를 처리, 추가 필요 데이터 보충
  - Transformation - 목적에 맞도록 차원 축소, 학습/검증 데이터 분리
  - Mining - 분석 기법 선택, 추가적인 전처리와 변환 과정
  - Interpretation/Evaluation - 해석과 평가, 목적 일치성 및 업무 활용도 평가

##### 3. CRISP-DM 분석

- 1996년 EU ESPRIT의 프로젝트에서 출발 (Daimler-Chrysler, SPSS, NCR, Teradata, OHRA 가 주도)
- Cross Industry Standard Process for Data Mining
  - 4단계 구조 (Phases - Generic Task - Specialized Task - Process Instance)로 점증적 세분화
  - 6단계 프로세스 (Biz Understanding - Data Understanding - ) 





### 3-2장 - 분석 마스터 플랜

### 3-2장 - 분석 마스터 플랜



## 4단원 - 데이터 분석

### 4-1장 데이터 분석 개요

### 4-2장 R프로그래밍 기초

### 4-3장 데이터 마트

#### 4-3-1절. 데이터 변경 및 요약

- reshape 패키지 - melt(행렬, id = c("기준1","기준2")) : unpivot / cast(행렬, 기준+기준~펼컬럼1~2) : pivot
- sqldf  패키지 - sqldf("sql문") 형태로 사용
- plyr 패키지 - 원본/대상에 따라 plyr 앞에 붙는 함수명이 달라진다 (데이터, "기준열", 함수)로 구성
- data.table - DF보다 빠름 (그룹핑/단문에서 특히 빠름) as.data.table(DF)로 만들고 setkey(DT,컬럼)등 설정

#### 4-3-2절. 데이터 가공

- Data Exploration - head, tail, summary 등
- 주요 변수 추출 - klaR 패키지 - 변수의 중요도 설명 plot을 그림. greedy.wilks() , plineplot(y~.,data,method,x)
- 변수 구간화 - 그룹핑이 필요한 경우. binning, 의사결정나무 활용

#### 4-3-3절. 기초 분석 및 데이터 관리

> summary()로 기초통계량 확인, is.na()/complete.cases() 등을 활용 NA값 확인 

- 결측값 대치 방법

  (NA 대치 - DMwR 패키지 knnimputation() : k이웃 값, centrallmputation() : 숫자 중위수, 요소 최빈값 대치)

  - 단순 대치 (삭제)
  - 평균 대치 (평균 값 or 회귀식을 통한 대치)
  - 단순 확률 대치 (표준 오차 과소추정 문제 해소 - Hot deck, nearest neighbor 방법)
  - 다중 확률 대치 (단순 확률 대치의 반복 - bootstapping based algorithm)

- 이상값 대치 방법 (오기입, 목적에 맞지 않음, 의도된 이상값 등을 처리)

  - 1. ESD(Extream Studentized Deviation) 기준 설정

    - EX_1 :  mean - 3sd < data < mean + 3sd까지
    - EX_2 : 기하평균 - 2.5 sd < data < 기하평균 + 2.5 sd
    - EX_3 : 사분위수 활용 : Q1 - 1.5(Q3-Q1) < data < Q3 + 1.5(Q3-Q1)

  - 2. 극단값 처리
       - 절단 : geo_mean, 상하단의 % 기준 제거
       - 조정 : 상하한 기준을 벗어나는 값을 상하한 값으로 조정

### 4-4장 통계분석

#### 4-4-1절. 통계분석 이해

- 표본 추출법 : 단순랜덤, 계통(구간별), 집락(군집별), 층화(군집별, 군집 크기의 비율 반영)
- 측정법과 척도 : 명목, 순서, 구간, 비율
- 추측통계 : 모수추정, 가설검정, 예측

##### 확률

- 확률 변수 (특정값이 나타날 가능성이 확률로 주어지는 변수, 정의역이 표본공간, 치역이 실수값(0<y<1))

- 2차 중심적률 : E[(X - m)**k] = 모분산 : 2차 적률 - 1차 적률 제곱

##### 확률분포

1. 이산형 변수의 분포

- 이산형 분포 - 0이 아닌 확률값을 갖는 확률 변수를 셀 수 있는 경우
- 베르누이 확률분포 - 결과가 2개만 나오는 경우
- 이항분포 - 베르누이 시행을 n번 반복했을 때, k번 성공할 확률 P(X = k)

- 기하분포 - 성공확률이 p인 베르누이 시행에서 첫번째 성공이 있기까지 x번 실패할 확률
- 다항분포 - 2개 초과의 결과를 가지는 반복 시행에서의 확률 분포 P(x1,x2...)
- 포아송분포 - 시공간 내에서 발생하는 사건의 횟수에 대한 확률분포 p(y)

2. 연속형 변수의 분포

- 균일분포 - 모든 X가 균일하게 발생하는 분포
- 정규분포 - 평균 mue, 표준편차 sigma인 x의 확률밀도함수
- 지수분포 - 어떤 사건이 발생할 때까지 경과 시간에 대한 연속확률분포
- t-분포 - 평균이 0을 중심으로 좌우가 동일한 분포를 따름. **두 집단의 평균이 동일한지 검정시 통계량으로 활용**
- x2분포 - 모평균, 모분산이 알려지지 않은 집단에 대한 가설 검정에 사용. **두 집단의 동질성 검정에 활용**
- F분포 - **두 집단간 분산의 동일성 검정에 활용**. 자유도가 커질수록 정규분포에 가까워진다.

##### 점추정과 구간추정

- 구간추정의 경우, 신뢰수준을 병기한다. 
  - 모분산을 알 경우 1.96 * 모표준변차 / 제곱근n 
  - 모분산을 모를 경우 2.26 * 표본표분편차 / 제곱근n

##### 가설검정

- 귀무가설(H0) - 비교하는 값과 차이가 없다. 동일하다를 전체하는 가설

- 대립가설(H1) - 반대 가설

- 유의수준(알파) - 귀무가설을 기각하는 확률크기 (옳은데 이를 기각하는(차이가 있다고 판단) 확률의 크기)

  (1종 오류의 크기) , 0.1, 0.05, 0.01 등으로 고정한다.

- 기각역 - 분포가 유의수준 알파인 부분 <=> 채택역

1. 모수적 검정
   - 모집단의 분포를 가정하고 그 가정의 검정통계량과 검정통계량의 분포를 유도해 검정 실시
2. 비모수적 검정
   - 자유도가 낮거나 서열관계를 나타내는 데이터에 활용 (모수에 제한을 두지 않음)
   - 단지 모수와의 분포가 동일한 형태인지만을 확인
   - 부호검정(sign test), 윌콕슨 순위합검정(rank sum test), 부호순위합검정 (wilcoxon signed rank test), 만 위트니의 U 검정, 런검정(run test), 스피어만 순위상관계수

#### 4-4-2절. 기초 통계분석

	##### 기술통계

- 중심위치, 산포를 측정할 수 있는 통계량
- 변동계수 (표본표준편차/표본평균), 평균표준오차 (표본표준편차/제곱근n) 
- 왜도(m3) (분포의 비대칭성 측도) - 양수일 때 : 왼쪽 밀집, 음수일 때 : 오른쪽 밀집
- 첨도(m4) (뾰족한 정도)  - 양수일 때 : 더 뾰족(정규분포보다), 음수일 때 : 덜 뾰족(정규분포보다)

##### 상관분석

- 상관계수 = 공분산 / x표준편차 * y표준편차 

- 피어슨 상관분석 - 등간척도 이상 (연속형 변수, 정규성 가정) - 계수 : 피어슨 r (적률상관계수)
- 스피어만 상관분석 - 서열척도 (순서형 변수, 비모수적 방법) - 계수 : 순위상관계수 ( p, 로우)

- 공분산 코드 : cov(x, y, method = ("pearson", "kendall", "spearman"))

- 상관관계 코드 : cor.test(x, y, method = ("pearson", "kendall", "spearman"))

  ​							rcorr(matrix(data), type =c("pearson", "kendall", "spearman")) : Hmisc 패키지

#### 4-4-3절. 통계분석 방법론



#### 4-4-4절. 회귀분석

#### 4-4-5절. 고급회귀

#### 4-4-6절. 시계열분석

#### 4-4-7절. 다차원척도법

#### 4-4-8절. 주성분분석



### 4-5장 데이터 마이닝



### 4-6장 비정형 데이터마이닝

## 5단원 - 데이터 시각화

 

### 5-1장 시각화 인사이트 프로세스



### 5-2장 시각화 디자인



### 5-3장 시각화 구현

