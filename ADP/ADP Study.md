# ADP Study



## 1단원 - 데이터의 이해

### 1-1장 - 데이터의 이해

#### 1-1-1절 데이터와 정보 (DIKW 정의)

**1. Data**

> 1646년 영국 문헌 첫 등장, 추론과 추정의 근거를 이루는 사실(옥스포드 대사전) 
>
> 데이터는 지식경영의 핵심인 암묵지의 형식지화에 역할을 담당한다 (Polany, 1966)

- 특성 - 존재적 (객관적 사실) / 당위적 (추론, 예측, 전망, 추정을 위한 근거)

- 유형 - 정성적 데이터 (언어, 문자) / 정량적 (수치, 도형, 기호 등)

**2. 정보**

> 데이터의 가공, 처리와 데이터간 연관관계 속에서 의미가 **도출된 것**

**3.지식**

> 도출된 정보를 구조화하여 유의미한 정보를 분류하고 경험을 결합시켜 **내재화한 것**

**4. 지혜**

> 지식의 축적과 아이디어가 결합된 창의적인 산물



#### 1-1-2절 DB의 정의 및 특징



**1. DataBase**

> 50년대 : 미군의 관리를 위한 컴퓨터 도서관을 설립 - 데이터의 기지라는 뜻의 DataBase 탄생
>
> 63년 : 미국 SDC가 개회한 심포지엄에서 공식 사용 '대량의 데이터를 축적하는 기지'의 의미
>
> 65년 : 2차 심포지엄에서 '시스템을 통한 체계적 관리와 저장 의미' 포함한 DB System 표현 등장
>
> 75년 : CAC가 KORSTIC을 통해 서비스됨. 우리나라에 DB  활용 시작 / TECHNOLINE 정보검색 서비스를 개시

- 정의 

  - 1차 개념 : 정형데이터의 관리
    - EU : 체계적, 조직적으로 정리되고 전자식 또는 기타 수단으로 개별적으로 접근할 수 있는 독립된 저작물, 데이터 또는 기타 소재의 수집물
    - 국내 : 소재를 체계적으로 배열 구성한 편집물. 개별적으로 접근하거나 검색할 수 있도록 한 것.
  - 2차 개념 : 빅데이터의 등장 (비정형 포함)
    - 국내 - 컴퓨터 용어사전 : 동시에 복수 업무 지원. 복수 이용자에 대응해 데이터를 받아들이고, 저장, 공급하기 위한 일정한 구조로 편성된 데이터의 집합
    - 국내 - 데이터분석 전문가 가이드 : 문자, 기호, 음성, 화상, 영상 등 상호 관련된 다수의 콘텐츠를 정보 처리 및 정보통신 기기에 의하여 체계적으로 수집, 축적하여 다양한 용도와 방법으로 이용할 수 있도록 정리한 정보의 집합체

- 특징

  - 통합된 (Integrated) - 중복 x
  - 저장된 (stored) - 저장 매채에 저장
  - 공용의 (shared) - 복수의 사용자가 복수의 목적으로 사용
  - 변화가능한 (changable) 

  - 정보 축적 및 전달 측면 : 기계가독성, 검색가독성, 원격조작성
  - 관리 및 이용 측면 : 일정한 질서와 구조에 따라 관리, 경제적
  - 정보기술 발전 측면 : 네트워크 기술의 발전에 이바지

#### 1-1-3절 DB 활용

- OLTP(On-line Transaction Processing) - 다양한 실시간 트랜젝션 데이터의 처리 프로세스
- OLAP(On-line Analytical Processing) - 요약된 데이터를 주제 중심적으로 활용하여 인사이트를 얻는 과정



### 1-2장 - 데이터의 가치와 미래

#### 1-2-1절 빅데이터의 이해

> 좁게는 3V의 특성을 가진 데이터를 의미, 기술과 분석까지 포함하거나 인재, 조직까지도 포함하는 단어 
>
> 4V - Volume(량), Variaty(다양성), Velocity(속도) + Value(가치), Visualization(시각화), Veracity(정확성)

#### 1-2-2절 빅데이터의 가치와 영향

> 맥킨지 曰 : 1. 투명성 제고(관리 효율 증대), 2. 시뮬레이션을 통한 예측(경쟁력 강화), 3. 세분화(맞춤 서비스), 					4. 알고리즘 활용 의사결정(경제성 증대), 5. BM과 서비스의 혁신

#### 1-2-3절 Business Model

> 연관규칙학습(상관관계), 유형분석(분류), 기계학습, 회귀분석(관계의 정량화), 감정분석, SNS분석 등

#### 1-2-4절 위기 요인과 통제 방안

> 위기 요인 : 사생활 침해, 책임원칙훼손(발생하지 않은 현상에 대한 확신), 데이터 오용
>
> 통제 방안 : 동의에서 책임으로, 결과 기반 책임 원칙, 알고리즘 접근 허용 



### 1-3장 - 데이터 사이언스와 전략 인사이트

#### 1-3-1절 분석과 전략 인사이트

> 과거의 대규모 투자에서 실패한 경험들이 아직 고착화되어 있음.
>
> 일차적인 분석보다는 전략적, 점증적 분석 전략을 활용해야함. 

#### 1-3-2절 인사이트 도출 필요 역량

> 데이터, 수학, 통계학, 컴퓨터공학, 시각화, 도메인 지식 등이 필요
>
> Analytics(분석), 컨설팅(스토리텔링, 시각화), IT 의 복합적 역량 필요 (Hard and Soft)

#### 1-3-3절 미래



## 2장 - 데이터 처리 기술의 이해

### 2-1장 - 데이터 처리 프로세스

#### 2-1-1절 ETL

> 데이터 이동 및 변환 (Extraction, Transformation, Load)
>
> ODS(operational Data Source)와 DW, DM을 만드는 과정을 의미
>
> Integration, Migration, Master Data Management에 활용되며 주기적 재사용이 가능한 컴포넌트들로 구성
>
> MPP(Masive Parallel Processing)를 활용 - 프로그램을 파트별로 나누어 병렬 처리하는 형식
>
> Batch ETL(일괄), Real Time ETL(실시간)로 구분

- 과정 : 스테이징 - 프로파일링(품질 측정, 특성 파악) - 클렌징 - integration - DeNormalizing - report table

![Process](.\img\Process.jpg)



** ODS란 ? ETL 과정을 위해 추출, 통합한 DB / DW로 이관됨 / 클렌징, 중복제거, 무결성 검증 등 수행 

​					/ Real Time 데이터를 관리할 목적으로 설계 

- ODS 생성과정
  - 인터페이스 단계
    - 원천으로 부터 데이터를 획득하는 과정
    - OLEDB(Object Linking and Embedding DataBase), ODBC(Object DataBase Connectivity), FTP(File Transfer Protocol) 등의 프로토콜 활용
  - 스테이징 단계
    - 스테이징 테이블에 저장되는 단계
    - 원천의 스키마에 의존하여 원본과의 일대일 혹은 일대다 형식의 테이블을 로드
    - RDB, 스프레드시트, Web, XML, 트랜젝션 데이터 등의 형태를 지님
  - 프로파일링 단계
    - 범위, 도메인, 유일성 확보 등의 규칙을 기준으로 데이터의 품질을 점검
    - 데이터 품질 보고서를 작성하는데 사용
  - 클렌징 단계
    - 오류 데이터들을 수정하는 단계
  - 인티그레이션 단계
    - 수정 완료 데이터를 ODS에 적재하는 단계
  - 익스포트
    - 보안 및 익스포트 규칙을 반영하여 DW, DM 등을 생성

** DW 란? ODS를 통해 정제 및 통합된 데이터가 분석 보고서 생성을 위해 적재되는 DB

​					주제중심적(최종사용자도 이해할 수 있는 형태를 지님), 영속성/비휘발성, 통합성, 시계열성

- DW의 형태 : 스타 스키마 (Fact - 3차원 정규화 , Dim - 2차원 정규화) - 중복 多, 조인 少, 구조 이해 쉬움

  ​					눈송이 스키마 (Fact, Dim 모두 3차원 정규화) - 중복 少, 조인 多, 구조 복잡



#### 2-1-2절 CDC (change Data Capture)

> 데이터 변경을 식별하고 후속처리를 자동화하는 기술 / 설계 기법
>
> 실시간 / 근접 실시간 서비스를 위해 구축 (Storage부터 App까지 다양한 계층에서 구축 가능)



- CDC 구현 기법 (푸쉬 방식과 풀 방식 중 선택)

  - Time Stamp on Row - Updata Time을 기록하고 체크

  - Version Number on Row - 버전을 기록하고 체크

  - Status on Row - 데이터 변경을 Boolean 값으로 기록하고 체크

  - Triggers on Table - 사전에 설정된 Trigger를 활용 (복잡도 증가, 변경 어려움, 확장성 감소로 주의 필요)

  - Event Programming - 변경 식별 기능을 App으로 구현하여 체크 - 복잡한만큼 복잡한 체크 가능

  - Log Scanner on DataBase - DB 트랜젝션 로그에 스캐닝을 구현 

    (스키마 변경 불필요, But DB의 종류가 다양할 경우 업무 복잡도가 높아질 수 있음)



#### 2-1-3절 EAI (Enterprise Application Integration)

> 비즈니스 프로세스 기반의 각종 Application을 상호연동을 하는 통합솔루션
>
> 데이터를 연계함으로서 동기화되도록 함
>
> Front_Office Sys, 레거시 Sys, 패키지 App 등의 형태로 산재된 App 프로세스 및 메시지를 통합 및 관리
>
> ETL이 배치 프로세스 중심이라면, EAI는 실시간 or 근접 실시간 위주
>
> Point To Point(복잡도 : N(N-1)/2개)를 Hub And Spoke(복잡도 : n) 방식으로 연계

- 구성
  - Adapter : 각 정보 시스템과 EAI 허브의 연결성을 확보
  - Bus : 연동 경로
  - Broker : 데이터 연동 규칙 통제
  - Transformer : 데이터 형식 변환 담당
- 구현 유형
  - Mediation(intra-communication)
    - EAI 엔진이 중개자로 동작하며, 데이터 갱신 및 생성 / 트랜젝션 완료 이벤트를 식별하여 동작
    - Publish / subscribe Model
  - Federation(inter-communication)
    - EAI 엔진이 외부 시스템으로부터 데이터 요청들을 일괄적으로 수령하여 필요한 데이터 전달
    - Repuest / Reply Model

** ESB(Enterprise Service Bus)와의 차이점

- Application 위주와 Process 위주의 차이 , 허브시스템 중앙집중형과 버스 형태의 느슨한 연결구조 차이



#### 2-1-4절 데이터 통합 및 연계 기법

> 모니터링이 필요한 센서 데이터 - Real Time, 대용량 전처리용 - Batch



#### 2-1-5절 대용량 비정형 데이터 처리

- Log 
  - 양이 방대, 오류 분석부터 마케팅까지 활용
  - 아파치 Flume NG, Facebook의 Scribe, 아파치 Chukwa 등 활용)
    - 실시간, 대상 서버 수만큼의 확장성 등 유연하고 고성능의 리소스 필요
    - 데이터 전송 시 보장이 무엇보다 중요. 성능과 안정성의 Trade-Off를 통해 알맞는 방식 설계 필요
    - 수집과 저장의 내장 플러그인을 제공 (하둡, NoSQL 저장 기능 포함)
    - 제공 기능의 일부 수정 용이

- 대규모 분산 병렬 처리
  - 하둡

### 2-2장 - 데이터 처리 기술



## 3단원 - 데이터 분석 기획

### 3-1장 - 데이터 분석 기획의 이해



### 3-2장 - 분석 마스터 플랜



## 4단원 - 데이터 분석

### 4-1장 데이터 분석 개요



### 4-2장 R프로그래밍 기초



### 4-3장 데이터 마트



### 4-4장 통계분석



### 4-5장 데이터 마이닝



### 4-6장 비정형 데이터마이닝

## 5단원 - 데이터 시각화

 

### 5-1장 시각화 인사이트 프로세스



### 5-2장 시각화 디자인



### 5-3장 시각화 구현