# ADP Study



## 1단원 - 데이터의 이해

### 1-1장 - 데이터의 이해

#### 1-1-1절 데이터와 정보 (DIKW 정의)

**1. Data**

> 1646년 영국 문헌 첫 등장, 추론과 추정의 근거를 이루는 사실(옥스포드 대사전) 
>
> 데이터는 지식경영의 핵심인 암묵지의 형식지화에 역할을 담당한다 (Polany, 1966)

- 특성 - 존재적 (객관적 사실) / 당위적 (추론, 예측, 전망, 추정을 위한 근거)

- 유형 - 정성적 데이터 (언어, 문자) / 정량적 (수치, 도형, 기호 등)

**2. 정보**

> 데이터의 가공, 처리와 데이터간 연관관계 속에서 의미가 **도출된 것**

**3.지식**

> 도출된 정보를 구조화하여 유의미한 정보를 분류하고 경험을 결합시켜 **내재화한 것**

**4. 지혜**

> 지식의 축적과 아이디어가 결합된 창의적인 산물



#### 1-1-2절 DB의 정의 및 특징



**1. DataBase**

> 50년대 : 미군의 관리를 위한 컴퓨터 도서관을 설립 - 데이터의 기지라는 뜻의 DataBase 탄생
>
> 63년 : 미국 SDC가 개회한 심포지엄에서 공식 사용 '대량의 데이터를 축적하는 기지'의 의미
>
> 65년 : 2차 심포지엄에서 '시스템을 통한 체계적 관리와 저장 의미' 포함한 DB System 표현 등장
>
> 75년 : CAC가 KORSTIC을 통해 서비스됨. 우리나라에 DB  활용 시작 / TECHNOLINE 정보검색 서비스를 개시

- 정의 

  - 1차 개념 : 정형데이터의 관리
    - EU : 체계적, 조직적으로 정리되고 전자식 또는 기타 수단으로 개별적으로 접근할 수 있는 독립된 저작물, 데이터 또는 기타 소재의 수집물
    - 국내 : 소재를 체계적으로 배열 구성한 편집물. 개별적으로 접근하거나 검색할 수 있도록 한 것.
  - 2차 개념 : 빅데이터의 등장 (비정형 포함)
    - 국내 - 컴퓨터 용어사전 : 동시에 복수 업무 지원. 복수 이용자에 대응해 데이터를 받아들이고, 저장, 공급하기 위한 일정한 구조로 편성된 데이터의 집합
    - 국내 - 데이터분석 전문가 가이드 : 문자, 기호, 음성, 화상, 영상 등 상호 관련된 다수의 콘텐츠를 정보 처리 및 정보통신 기기에 의하여 체계적으로 수집, 축적하여 다양한 용도와 방법으로 이용할 수 있도록 정리한 정보의 집합체

- 특징

  - 통합된 (Integrated) - 중복 x
  - 저장된 (stored) - 저장 매채에 저장
  - 공용의 (shared) - 복수의 사용자가 복수의 목적으로 사용
  - 변화가능한 (changable) 

  - 정보 축적 및 전달 측면 : 기계가독성, 검색가독성, 원격조작성
  - 관리 및 이용 측면 : 일정한 질서와 구조에 따라 관리, 경제적
  - 정보기술 발전 측면 : 네트워크 기술의 발전에 이바지

#### 1-1-3절 DB 활용

- OLTP(On-line Transaction Processing) - 다양한 실시간 트랜젝션 데이터의 처리 프로세스
- OLAP(On-line Analytical Processing) - 요약된 데이터를 주제 중심적으로 활용하여 인사이트를 얻는 과정



### 1-2장 - 데이터의 가치와 미래

#### 1-2-1절 빅데이터의 이해

> 좁게는 3V의 특성을 가진 데이터를 의미, 기술과 분석까지 포함하거나 인재, 조직까지도 포함하는 단어 
>
> 4V - Volume(량), Variaty(다양성), Velocity(속도) + Value(가치), Visualization(시각화), Veracity(정확성)

#### 1-2-2절 빅데이터의 가치와 영향

> 맥킨지 曰 : 1. 투명성 제고(관리 효율 증대), 2. 시뮬레이션을 통한 예측(경쟁력 강화), 3. 세분화(맞춤 서비스), 					4. 알고리즘 활용 의사결정(경제성 증대), 5. BM과 서비스의 혁신

#### 1-2-3절 Business Model

> 연관규칙학습(상관관계), 유형분석(분류), 기계학습, 회귀분석(관계의 정량화), 감정분석, SNS분석 등

#### 1-2-4절 위기 요인과 통제 방안

> 위기 요인 : 사생활 침해, 책임원칙훼손(발생하지 않은 현상에 대한 확신), 데이터 오용
>
> 통제 방안 : 동의에서 책임으로, 결과 기반 책임 원칙, 알고리즘 접근 허용 



### 1-3장 - 데이터 사이언스와 전략 인사이트

#### 1-3-1절 분석과 전략 인사이트

> 과거의 대규모 투자에서 실패한 경험들이 아직 고착화되어 있음.
>
> 일차적인 분석보다는 전략적, 점증적 분석 전략을 활용해야함. 

#### 1-3-2절 인사이트 도출 필요 역량

> 데이터, 수학, 통계학, 컴퓨터공학, 시각화, 도메인 지식 등이 필요
>
> Analytics(분석), 컨설팅(스토리텔링, 시각화), IT 의 복합적 역량 필요 (Hard and Soft)

#### 1-3-3절 미래



## 2장 - 데이터 처리 기술의 이해

### 2-1장 - 데이터 처리 프로세스

#### 2-1-1절 ETL

> 데이터 이동 및 변환 (Extraction, Transformation, Load)
>
> ODS(operational Data Source)와 DW, DM을 만드는 과정을 의미
>
> Integration, Migration, Master Data Management에 활용되며 주기적 재사용이 가능한 컴포넌트들로 구성
>
> MPP(Masive Parallel Processing)를 활용 - 프로그램을 파트별로 나누어 병렬 처리하는 형식
>
> Batch ETL(일괄), Real Time ETL(실시간)로 구분

- 과정 : 스테이징 - 프로파일링(품질 측정, 특성 파악) - 클렌징 - integration - DeNormalizing - report table

![Process](.\img\Process.jpg)



** ODS란 ? ETL 과정을 위해 추출, 통합한 DB / DW로 이관됨 / 클렌징, 중복제거, 무결성 검증 등 수행 

​					/ Real Time 데이터를 관리할 목적으로 설계 

- ODS 생성과정
  - 인터페이스 단계
    - 원천으로 부터 데이터를 획득하는 과정
    - OLEDB(Object Linking and Embedding DataBase), ODBC(Object DataBase Connectivity), FTP(File Transfer Protocol) 등의 프로토콜 활용
  - 스테이징 단계
    - 스테이징 테이블에 저장되는 단계
    - 원천의 스키마에 의존하여 원본과의 일대일 혹은 일대다 형식의 테이블을 로드
    - RDB, 스프레드시트, Web, XML, 트랜젝션 데이터 등의 형태를 지님
  - 프로파일링 단계
    - 범위, 도메인, 유일성 확보 등의 규칙을 기준으로 데이터의 품질을 점검
    - 데이터 품질 보고서를 작성하는데 사용
  - 클렌징 단계
    - 오류 데이터들을 수정하는 단계
  - 인티그레이션 단계
    - 수정 완료 데이터를 ODS에 적재하는 단계
  - 익스포트
    - 보안 및 익스포트 규칙을 반영하여 DW, DM 등을 생성

** DW 란? ODS를 통해 정제 및 통합된 데이터가 분석 보고서 생성을 위해 적재되는 DB

​					주제중심적(최종사용자도 이해할 수 있는 형태를 지님), 영속성/비휘발성, 통합성, 시계열성

- DW의 형태 : 스타 스키마 (Fact - 3차원 정규화 , Dim - 2차원 정규화) - 중복 多, 조인 少, 구조 이해 쉬움

  ​					눈송이 스키마 (Fact, Dim 모두 3차원 정규화) - 중복 少, 조인 多, 구조 복잡



#### 2-1-2절 CDC (change Data Capture)

> 데이터 변경을 식별하고 후속처리를 자동화하는 기술 / 설계 기법
>
> 실시간 / 근접 실시간 서비스를 위해 구축 (Storage부터 App까지 다양한 계층에서 구축 가능)



- CDC 구현 기법 (푸쉬 방식과 풀 방식 중 선택)

  - Time Stamp on Row - Updata Time을 기록하고 체크

  - Version Number on Row - 버전을 기록하고 체크

  - Status on Row - 데이터 변경을 Boolean 값으로 기록하고 체크

  - Triggers on Table - 사전에 설정된 Trigger를 활용 (복잡도 증가, 변경 어려움, 확장성 감소로 주의 필요)

  - Event Programming - 변경 식별 기능을 App으로 구현하여 체크 - 복잡한만큼 복잡한 체크 가능

  - Log Scanner on DataBase - DB 트랜젝션 로그에 스캐닝을 구현 

    (스키마 변경 불필요, But DB의 종류가 다양할 경우 업무 복잡도가 높아질 수 있음)



#### 2-1-3절 EAI (Enterprise Application Integration)

> 비즈니스 프로세스 기반의 각종 Application을 상호연동을 하는 통합솔루션
>
> 데이터를 연계함으로서 동기화되도록 함
>
> Front_Office Sys, 레거시 Sys, 패키지 App 등의 형태로 산재된 App 프로세스 및 메시지를 통합 및 관리
>
> ETL이 배치 프로세스 중심이라면, EAI는 실시간 or 근접 실시간 위주
>
> Point To Point(복잡도 : N(N-1)/2개)를 Hub And Spoke(복잡도 : n) 방식으로 연계

- 구성
  - Adapter : 각 정보 시스템과 EAI 허브의 연결성을 확보
  - Bus : 연동 경로
  - Broker : 데이터 연동 규칙 통제
  - Transformer : 데이터 형식 변환 담당
- 구현 유형
  - Mediation(intra-communication)
    - EAI 엔진이 중개자로 동작하며, 데이터 갱신 및 생성 / 트랜젝션 완료 이벤트를 식별하여 동작
    - Publish / subscribe Model
  - Federation(inter-communication)
    - EAI 엔진이 외부 시스템으로부터 데이터 요청들을 일괄적으로 수령하여 필요한 데이터 전달
    - Repuest / Reply Model

** ESB(Enterprise Service Bus)와의 차이점

- Application 위주와 Process 위주의 차이 , 허브시스템 중앙집중형과 버스 형태의 느슨한 연결구조 차이



#### 2-1-4절 데이터 통합 및 연계 기법

> 모니터링이 필요한 센서 데이터 - Real Time, 대용량 전처리용 - Batch



#### 2-1-5절 대용량 비정형 데이터 처리

- Log 
  - 양이 방대, 오류 분석부터 마케팅까지 활용
  - 아파치 Flume NG, Facebook의 Scribe, 아파치 Chukwa 등 활용)
    - 실시간, 대상 서버 수만큼의 확장성 등 유연하고 고성능의 리소스 필요
    - 데이터 전송 시 보장이 무엇보다 중요. 성능과 안정성의 Trade-Off를 통해 알맞는 방식 설계 필요
    - 수집과 저장의 내장 플러그인을 제공 (하둡, NoSQL 저장 기능 포함)
    - 제공 기능의 일부 수정 용이

- 대규모 분산 병렬 처리
  - 하둡

### 2-2장 - 데이터 처리 기술

#### 2-2-1절 분산 데이터 저장 기술

> 분산 파일 시스템, 클러스터, DB, NoSQL

##### 1. 분산 파일 시스템

> 분산된 서버들로 구성, 대규모 클러스터 시스템 플랫폼 : 저장 공간, 처리 성능, 확장성, 신뢰성, 가용성 확보
>
> 파일의 메타데이터를 관리하는 전용 서버 + 데이터 저장 처리 서버 : 비대칭형 클러스터 파일 시스템

- GFS (Google File System) - 64MB 청크로 분산, 헤시 테이블 구조 활용, 마스터에 의해 생성/삭제 구별
  - 가정 : 고장이 빈번한 저가형 서버 다수 운용, 쓰기가 순차적이며 갱신은 드뭄. 실시간보다 높은 처리율
  - 클라이언트(POSIX 지원 X) - 마스터(메타데이터를 메모리 상에서 처리) - 청크 서버(하트비트 송출) 
  - 클라이언트는 마스터에게 핸들을 받고 청크 서버에 직접 요청
- HDFS (Hadoop File System) - 아파치 너치 프로젝트를 클로닝, 128MB 블록 구성
  - 네임노드 - 데이터노드 - 보조네임노드
  - 순차적 스트리밍 방식으로 저장 조회(노드 1 완료 후 2실행, 3실행 순), 배치 작업에 적합 
  - 클라이언트가 마스터에게 정보를 제공받아 직접 데이터를 읽어 들임.
- 러스터 - 객체 기반 클러스터 파일 시스템
  - 고속 네트워크로 연결된 클라이언트 서버(리눅스 VFS) - 메타 서버 - 객체 저장 서버(스트라이핑 기법 사용)
  - 계층화된 모듈 구조로 TCP/IP, 인피니밴드, 미리넷을 지원
  - 유닉스 시멘틱 제공, 라이트백 캐시 지원, 클라이언트에서 갱신 레코드를 생성, 무결성을 위해 잠금 활용
  - 인텐트 기반 잠금 프로토콜 활용

##### 2. DB 클러스터

> 클러스터링 - 하나의 DB를 여러 서버에 분산 배치, 파티셔닝 - 하나의 DB를 분할하여 처리하는 방식
>
> 병렬처리, 고가용성을 위해 활용, 무공유/공유 디스크로 구분

- 무공유 디스크 - 대부분의 클러스터가 채택, 서로 완전히 분리되어 서로의 데이터에 소유권 X
  - 노드확장에 제한이 없으나 폴트톨로런스를 구성해야함
- 공유 디스크 - 각 인스턴스가 모든 데이터에 접근 가능, SAN(Storage Area Network)등의 활용
  - 폴트톨로런스를 제공(하나의 서버만 살아도 동작), 확장이 제한적이고 병목이 발생, 동기화 방식 고민 필요



----

----

띄어 넘음

---



## 3단원 - 데이터 분석 기획

### 3-1장 - 데이터 분석 기획의 이해

#### 3-1-1절 분석기획 방향성 도출

> IT, 수리통계, 도메인 지식을 기반으로 What, Why, How를 구상.
>
> 대상O 분석O - Optimization, 대상X, 분석O - Insight, 대상O, 분석X - Solution, 대상X, 분석X - Discovery
>
> 가용 데이터, 목적, 한계 등을 최대한 고려하여 작성

#### 3-1-2절 분석 방법론

> 방법론 - 절차, 방법, 도구와 기법, 템플릿과 산출물로 구성
>
> 암묵지 (형식화) 형식지 (체계화) 방법론 (내재화) 암묵지 순환 [공통화, 내면화] [표출화, 연결화]



##### 1. 방법론의 적용 모델

- 폭포수 모델 - 단계를 순차적으로 수행하는 모델 

- 프로토타입 모델 - 고객 요구가 뚜렷하지 않을 때 활용. 프로토타입을 재공하고 개선해나가는 방식

- 나선형 모델 - 점증적 개발. 바탕이 없는 상황에서 활용가능하지만 체계가 잡히지 않으면 어려움을 동반



##### 2. KDD 분석

- 1996년 Fayyad가 프로파일링 기술을 기반으로 정리 (Knowledge Discovery in DataBases)
  - Selection - 도메인 및 프로젝트 목표 이해, 목표 데이터의 선정
  - PreProcessing - Noise, Outlier, Missing Value 를 처리, 추가 필요 데이터 보충
  - Transformation - 목적에 맞도록 차원 축소, 학습/검증 데이터 분리
  - Mining - 분석 기법 선택, 추가적인 전처리와 변환 과정
  - Interpretation/Evaluation - 해석과 평가, 목적 일치성 및 업무 활용도 평가

##### 3. CRISP-DM 분석

- 1996년 EU ESPRIT의 프로젝트에서 출발 (Daimler-Chrysler, SPSS, NCR, Teradata, OHRA 가 주도)
- Cross Industry Standard Process for Data Mining
  - 4단계 구조 (Phases - Generic Task - Specialized Task - Process Instance)로 점증적 세분화
  - 6단계 프로세스 (Biz Understanding - Data Understanding - ) 





### 3-2장 - 분석 마스터 플랜

### 3-2장 - 분석 마스터 플랜



---

## 4단원 - 데이터 분석

### 4-1장 데이터 분석 개요

### 4-2장 R프로그래밍 기초

### 4-3장 데이터 마트

---

#### 4-3-1절. 데이터 변경 및 요약

- reshape 패키지 - melt(행렬, id = c("기준1","기준2")) : unpivot / cast(행렬, 기준+기준~펼컬럼1~2) : pivot
- sqldf  패키지 - sqldf("sql문") 형태로 사용
- plyr 패키지 - 원본/대상에 따라 plyr 앞에 붙는 함수명이 달라진다 (데이터, "기준열", 함수)로 구성
- data.table - DF보다 빠름 (그룹핑/단문에서 특히 빠름) as.data.table(DF)로 만들고 setkey(DT,컬럼)등 설정

---

#### 4-3-2절. 데이터 가공

- Data Exploration - head, tail, summary 등
- 주요 변수 추출 - klaR 패키지 - 변수의 중요도 설명 plot을 그림. greedy.wilks() , plineplot(y~.,data,method,x)
- 변수 구간화 - 그룹핑이 필요한 경우. binning, 의사결정나무 활용

---

#### 4-3-3절. 기초 분석 및 데이터 관리

> summary()로 기초통계량 확인, is.na()/complete.cases() 등을 활용 NA값 확인 

- 결측값 대치 방법

  (NA 대치 - DMwR 패키지 knnimputation() : k이웃 값, centrallmputation() : 숫자 중위수, 요소 최빈값 대치)

  - 단순 대치 (삭제)
  - 평균 대치 (평균 값 or 회귀식을 통한 대치)
  - 단순 확률 대치 (표준 오차 과소추정 문제 해소 - Hot deck, nearest neighbor 방법)
  - 다중 확률 대치 (단순 확률 대치의 반복 - bootstapping based algorithm)

- 이상값 대치 방법 (오기입, 목적에 맞지 않음, 의도된 이상값 등을 처리)

  - 1. ESD(Extream Studentized Deviation) 기준 설정

    - EX_1 :  mean - 3sd < data < mean + 3sd까지
    - EX_2 : 기하평균 - 2.5 sd < data < 기하평균 + 2.5 sd
    - EX_3 : 사분위수 활용 : Q1 - 1.5(Q3-Q1) < data < Q3 + 1.5(Q3-Q1)

  - 2. 극단값 처리
       - 절단 : geo_mean, 상하단의 % 기준 제거
       - 조정 : 상하한 기준을 벗어나는 값을 상하한 값으로 조정

---

### 4-4장 통계분석

---

#### 4-4-1절. 통계분석 이해

- 표본 추출법 : 단순랜덤, 계통(구간별), 집락(군집별), 층화(군집별, 군집 크기의 비율 반영)
- 측정법과 척도 : 명목, 순서, 구간, 비율
- 추측통계 : 모수추정, 가설검정, 예측

##### 확률

- 확률 변수 (특정값이 나타날 가능성이 확률로 주어지는 변수, 정의역이 표본공간, 치역이 실수값(0<y<1))

- 2차 중심적률 : E[(X - m)**k] = 모분산 : 2차 적률 - 1차 적률 제곱

##### 확률분포

1. 이산형 변수의 분포

- 이산형 분포 - 0이 아닌 확률값을 갖는 확률 변수를 셀 수 있는 경우
- 베르누이 확률분포 - 결과가 2개만 나오는 경우
- 이항분포 - 베르누이 시행을 n번 반복했을 때, k번 성공할 확률 P(X = k)

- 기하분포 - 성공확률이 p인 베르누이 시행에서 첫번째 성공이 있기까지 x번 실패할 확률
- 다항분포 - 2개 초과의 결과를 가지는 반복 시행에서의 확률 분포 P(x1,x2...)
- 포아송분포 - 시공간 내에서 발생하는 사건의 횟수에 대한 확률분포 p(y)

2. 연속형 변수의 분포

- 균일분포 - 모든 X가 균일하게 발생하는 분포
- 정규분포 - 평균 mue, 표준편차 sigma인 x의 확률밀도함수
- 지수분포 - 어떤 사건이 발생할 때까지 경과 시간에 대한 연속확률분포
- t-분포 - 평균이 0을 중심으로 좌우가 동일한 분포를 따름. **두 집단의 평균이 동일한지 검정시 통계량으로 활용**
- x2분포 - 모평균, 모분산이 알려지지 않은 집단에 대한 가설 검정에 사용. **두 집단의 동질성 검정에 활용**
- F분포 - **두 집단간 분산의 동일성 검정에 활용**. 자유도가 커질수록 정규분포에 가까워진다.

##### 점추정과 구간추정

- 구간추정의 경우, 신뢰수준을 병기한다. 
  - 모분산을 알 경우 1.96 * 모표준변차 / 제곱근n 
  - 모분산을 모를 경우 2.26 * 표본표분편차 / 제곱근n

##### 가설검정

- 귀무가설(H0) - 비교하는 값과 차이가 없다. 동일하다를 전체하는 가설

- 대립가설(H1) - 반대 가설

- 유의수준(알파) - 귀무가설을 기각하는 확률크기 (옳은데 이를 기각하는(차이가 있다고 판단) 확률의 크기)

  (1종 오류의 크기) , 0.1, 0.05, 0.01 등으로 고정한다.

- 기각역 - 분포가 유의수준 알파인 부분 <=> 채택역

1. 모수적 검정
   - 모집단의 분포를 가정하고 그 가정의 검정통계량과 검정통계량의 분포를 유도해 검정 실시
2. 비모수적 검정
   - 자유도가 낮거나 서열관계를 나타내는 데이터에 활용 (모수에 제한을 두지 않음)
   - 단지 모수와의 분포가 동일한 형태인지만을 확인
   - 부호검정(sign test), 윌콕슨 순위합검정(rank sum test), 부호순위합검정 (wilcoxon signed rank test), 만 위트니의 U 검정, 런검정(run test), 스피어만 순위상관계수

---

#### 4-4-2절. 기초 통계분석

##### 1. 기술통계

- 중심위치, 산포를 측정할 수 있는 통계량
- 변동계수 (표본표준편차/표본평균), 평균표준오차 (표본표준편차/제곱근n) 
- 왜도(m3) (분포의 비대칭성 측도) - 양수일 때 : 왼쪽 밀집, 음수일 때 : 오른쪽 밀집
- 첨도(m4) (뾰족한 정도)  - 양수일 때 : 더 뾰족(정규분포보다), 음수일 때 : 덜 뾰족(정규분포보다)

##### 2. 상관분석

- 상관계수 = 공분산 / x표준편차 * y표준편차 

- 피어슨 상관분석 - 등간척도 이상 (연속형 변수, 정규성 가정) - 계수 : 피어슨 r (적률상관계수)
- 스피어만 상관분석 - 서열척도 (순서형 변수, 비모수적 방법) - 계수 : 순위상관계수 ( p, 로우)

- 공분산 코드 : cov(x, y, method = ("pearson", "kendall", "spearman"))

- 상관관계 코드 : cor.test(x, y, method = ("pearson", "kendall", "spearman"))

  ​							rcorr(matrix(data), type =c("pearson", "kendall", "spearman")) : Hmisc 패키지

#### 4-4-3절. 통계분석 방법론

##### 1. T검정

- 일표본 T검정 - 모집단의 정규분포성을 가정. 모집단에서 추출한 연속형 변수와 모집단의 비교

  EX_한 과수원의 사과에서 재배되는 과실의 평균 무게가 200kg이 맞는가 확인

  - 실행 단계
    - 모평균을 모수로 한다.
    - 귀무가설, 대립가설을 설정
    - 유의수준 설정
    - 검정통계량의 값 및 유의확률 계산 t = 표본평균 - 모평균 / (표본 표준편차 / 제곱근n)
    - 기각여부 판단 및 의사결정
  - R 실행 단계
    - shapiro.test(data)를 통해 정규분포를 따르는지 확인
    - t.test(x, alternative = c("two.sided","less","greater"),mu)로 실행

- 대응표본 T검정 - 모집단 정규분포 가정. 모집단에 서로 다른 두 처리를 했을 때, 두 처리에 대한 평균 차이 비교

  (동일한 개체에 대해 2번의 처리를 가하고, 각 개체를 2번 측정한다.)

  EX_ 영양제 투여 전후의 평균 수명시간에 차이가 있는지 확인  

  - 실행 단계
    - 두 개의 모평균의 차이를 모수로 한다.
    - 귀무가설, 대립가설을 설정
    - 유의수준 설정
    - 검정통계량의 값 및 유의확률 계산 - t = 차이 평균-(처리x평균 - 처리y평균) / (차이 표준편차/제곱급n)
    - 기각여부 판단 및 의사결정

  - R 실행 단계
    - 정규분포 가정 확인 및 데이터셋 준비
    - t.test(x, y, alternative, paired = True) 실행

- 독립표본 T검정 - 두 모집단의 정규분포 및 등분산성, 독립성을 가정. 두 개의 모집단의 평균을 비교

  **(독립변수는 범주형, 종속변수는 연속형이어야 한다.)**

  EX_성별에 따라 출근시간에 차이가 있는가

  - 실행 단계

    - 서로 독립된 두 모집단의 평균을 모수로 한다.
    - 귀무가설, 대립가설을 설정
    - 유의수준 설정
    - 등분산 검정
    - 검정통계량의 값 및 유의확률 계산 t = (표본1평균-표본2평균)-두 모평균 차이값/ (X1-X2 표준오차) 
    - 기각여부 판단 및 의사결정

  - R 실행

    - var.test(formula, data, alternative="two.sided")로 등분산 검사 수행

    - t.test(formula, data, alternative, var.equal = True)로 실행

      이 때, formula의 종속변수~독립변수는 하나의 DF에 존재하며, unpivot된 형태로 존재

##### 2. ANOVA (분산분석)

> 두 개 이상의 집단들의 평균 간 차이 비교 (그룹 내의 변동에 비교하여)
>
> 그룹을 구분하는 기준변수는 반드시 factor형이어야 함
>
> 서로 독립적이고 정규분포를 따르며, 등분산성을 가지는 집단을 가정



- 일변량 : 일원배치 분산분석 (독립 1 : 종속 1)

  반응값에 대해 범주형 변수 1개의 영향을 알아보기 위한 방법. F검정 통계량 활용

  

  - 통계 모형
    $$
    y{_i}{_j} = \mu{_i} + \epsilon{_i}{_j}, \epsilon{_i}{_j} \sim N(0,\sigma{^2})
    $$

  - 분산분석표

  | 요인 | 제곱합(SS) |     자유도(df)      | 평균제곱(MS) |   분산비(F)   |
  | :--: | :--------: | :-----------------: | :----------: | :-----------: |
  | 처리 |    SSA     | k-1 (k = 집단의 수) |     MSA      | F = MSA / MSE |
  | 오차 |    SSE     | N-k (N = 관측의 수) |     MSE      |               |
  | 전체 |    SST     |         N-1         |              |               |

  ** SSA : 집단이 가지는 변동제곱합 												SSE : 오차가 가지는 변동제곱합

  ** SST : 관측치 값이 가지는 변동제곱합 (SSA + SSE)					MSA : SSA / k-1

  

  - R 실행
    - restult <- aov(formula, data)를 실행하여 변수에 결과를 저장
    - summary(result)를 실행하면 분산분석표와 신뢰도 확인 가능
  - 결과 분석 및 의사결정
    - result 확인 결과 종속변수 그룹간 독립변수가 다르다는 점이 확인된 경우.
    - TukeyHSD,Duncan,MRT(Multiple Range Test),피셔(Fisher) 유의차(LSD),Scheff 등 활용 사후검증

- 일변량 : 이원배치 분산분석 (독립 2 : 종속 1)

  두 개의 범주형 변수 A,B의 영향을 검증 - A와 B의 교호작용 검증이 필요

  - 통계 모형

  $$
  y{_i}{_j}{_k} = \mu + \alpha{_i} + \beta{_j} + (\alpha\beta){_i}{_j} + \epsilon{_i}{_j}{_k}
  $$

  - 분산분석표

    | 요인     | 제곱합 | 자유도     | 평균제곱합               | F                  |
    | -------- | ------ | ---------- | ------------------------ | ------------------ |
    | A        | SSA    | I-1        | MSA = SSA / I-1          | F(a) = MSA / MSE   |
    | B        | SSB    | J-1        | MSB = SSB / J-1          | F(b) = MSB / MSE   |
    | 상호작용 | SSAB   | (I-1)(J-1) | MSAB = SSAB / (I-1)(J-1) | F(ab) = MSAB / MSE |
    | 오차     | SSE    | IJ(n-1)    | MSE = SSE / IJ(n-1)      |                    |
    | 전체     | SST    | IJn-1      |                          |                    |

- 일변량 : 다원배치 분산분석 (독립 n : 종속 1)

- 다변량 : MANOVA (독립 1 : 종속 n)

- 실험계획법 (DOE, Design Of Experiment)

  영향을 미치는 인자를 도출, 측정 데이터를 통계적으로 분석하기 위한 실험을 설계하는 방법

  - 목적
    - 분산분석 및 검정과 추정 : 유의미한 요인과 영향 정도 파악
    - 최적 반응 조건의 결정 : 어떤 인자를 사용해야 가장 원하는 결과를 얻을지 확인
    - 오차항 추정 : 이해하기 어렵던 오차와 그 변동에 관한 정도 파악
  - 원리
    - 랜덤화(실험 순서 무작위), 반복(동일 수준으로 2번 이상 실행), 블록화(시공간적 분할 블록화) 직교화(요인간 직교성 확인), 교락
  - 주요 용어
    - 인자(Factor) : 실험 대상 X
    - 특성치(Characteristic Value) : 결과값 Y
    - 수준(Level) : 인자의 조건 값
    - 주효과(Main Effect) : 수준간 차이나 인자가 미치는 영향
    - 교호효과(Interaction Effect) : 혼합 영향 정도
    - 교락(Confounding)
    - 블록(Block) : 실험단위의 균일성을 위해 단위를 모은 것
    - 반복(Replication), 중복(Repetition)
  - 종류
    - 요인배치법(모든 수준의 조합에서 실험) - 완전랜덤화방법 (K**n 실험)
    - 분할법(단계로 분할)
    - 교락법(검출할 필요가 없는 교호작용을 다른 요인과 교락하도록 배치)
    - 난괴법(실험 단위를 몇 개의 반복으로 나누어 배치)

##### 3. 교차분석

> 범주형 자료인 두 변수 간의 관계를 분석
>
> **적합도 검정, 독립성 검정, 동일성 검정에 사용됨**
>
> **카이제곱 검정 통계량을 활용**

- 교차표 - 두 변수의 각 범주를 교차하여 데이터의 도수를 표 형태로 표시 - 각 셀의 관찰/기대빈도 차이 검정

- 적합도 검정

  - 귀무가설 : 관측 교차표(실제 분포)와 기대 교차표(이론적 분포)의 차이가 없다. 귀무가설 : 있다.
  - 카이 제곱

  $$
  X{^2} = \sum_{i=1}^k \frac{(O{_i}-E{_i})^2}{E{_i}}
  $$

  - 카이제곱 값이 크면 (기대치와 실제치의 간극이 크다는 의미
  - R 실행 : chisq.test(x,y,p) : p는 p = c(0.2,0.8) 처럼 귀무가설에서 정한 x 그룹의 비율을 의미

- 독립성 검정

  - 모 집단을 범주화한 2개의 변수가 서로 독립인지 확인

  - 귀무가설 : 두 변수는 관계가 없다. 대립다설 : 두 변수는 관계가 있다.

  - 카이제곱 ( n:관측도수,r:행수,c:열수, O(i) : 행의합 , O(j) : 열의합, O(ij) : 관찰빈도, E(ij) : O(i) * O(j) / n)
    $$
    X{^2} = \sum_{i=1}^r\sum_{j=1}^c\frac{(O{_i}{_j}-E{_i}{_j})^2}{E{_i}{_j}}
    $$

- 동질성 검정

  - 모집단이 임의의 변수에 따라 R개의 속성으로 범주화되었을 때, R개의 부분 모집단에서 추출한 각 표본인 C개의 범주화된 집단의 분포는 서로 동일한지 아닌지를 검정하는 것
  - 계산 과정은 독립성 검정과 동일하다. 

##### 4. 중심극한정리

> 표본이 커질수록 정규분포에 가까워지는 현상



---

#### 4-4-4절. 회귀분석

> 독립변수들이 종속변수에 미치는 영향을 추정하는 기법
>
> 독립변수의 개수에 따라 단순선형회귀, 다중선형회귀로 나눔

- 종속변수, 반응변수, 결과변수 : 독립변수, 설명변수, 예측변수
- 기본 가정
  - 선형성 : 가장 중요한 가정. 독립변수와 종속변수의 관계가 선형.
  - 등분산성 : 입력변수에 상관없이 오차가 일정함.
  - 독립성 : 자기상관이 없어야 함. Durbin-Waston 통계량을 주로 사용하며 시계열 데이터에서 활용됨.
  - 비상관성 : 오차들끼리 상관이 없음.
  - 정상성 : 오차의 분포가 정규분포를 따름 (Q-Q plot, Kolmogolov-Smirnov검정, Shaprio-Wilk검정 활용)



#### 4-4-5절. 고급회귀

#### 4-4-6절. 시계열분석

#### 4-4-7절. 다차원척도법

#### 4-4-8절. 주성분분석



### 4-5장 데이터 마이닝



### 4-6장 비정형 데이터마이닝

## 5단원 - 데이터 시각화

 

### 5-1장 시각화 인사이트 프로세스



### 5-2장 시각화 디자인



### 5-3장 시각화 구현

